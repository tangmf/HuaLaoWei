{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8318cb2b",
   "metadata": {},
   "source": [
    "## Training classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6465ed8",
   "metadata": {},
   "source": [
    "Using SmolVLM for extracting text and image embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3f8746",
   "metadata": {},
   "source": [
    "Can expand to Qwen2.5VM if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ad2af1e",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\n",
      "Built on Fri_Feb_21_20:42:46_Pacific_Standard_Time_2025\n",
      "Cuda compilation tools, release 12.8, V12.8.93\n",
      "Build cuda_12.8.r12.8/compiler.35583870_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8ddc668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jerick\\Documents\\GitHub\\FixMyStreetDataset\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, AutoModelForImageTextToText\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import base64\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004ad29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/2. Sorted/Abandoned Trolleys/Other Trolleys/4156788.0.full.jpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29ad056",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/2. Sorted/Roads & Footprints/Faulty Steetlight/5174732.0.full.jpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b66504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/2. Sorted/Housing/Playground & Fitness Facilities Maintenance/4223509.0.full.jpeg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8045f",
   "metadata": {},
   "source": [
    "## SmolVLM2 Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c5bf80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [05:38<00:00, 169.25s/it]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingFaceTB/SmolVLM2-2.2B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForImageTextToText\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_attn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflash_attention_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jerick\\Documents\\GitHub\\FixMyStreetDataset\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    572\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    573\u001b[0m     )\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Jerick\\Documents\\GitHub\\FixMyStreetDataset\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mc:\\Users\\Jerick\\Documents\\GitHub\\FixMyStreetDataset\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:4336\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4334\u001b[0m config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[0;32m   4335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_attn_implementation_autoset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 4336\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autoset_attn_implementation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[0;32m   4338\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4340\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[0;32m   4341\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m   4342\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jerick\\Documents\\GitHub\\FixMyStreetDataset\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:2109\u001b[0m, in \u001b[0;36mPreTrainedModel._autoset_attn_implementation\u001b[1;34m(cls, config, use_flash_attention_2, torch_dtype, device_map, check_device_map)\u001b[0m\n\u001b[0;32m   2106\u001b[0m     config\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 2109\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_and_enable_flash_attn_2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhard_check_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2115\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m requested_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflex_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2117\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_enable_flex_attn(config, hard_check_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Jerick\\Documents\\GitHub\\FixMyStreetDataset\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:2252\u001b[0m, in \u001b[0;36mPreTrainedModel._check_and_enable_flash_attn_2\u001b[1;34m(cls, config, torch_dtype, device_map, check_device_map, hard_check_only)\u001b[0m\n\u001b[0;32m   2250\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[0;32m   2251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2252\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m the package flash_attn seems to be not installed. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2254\u001b[0m flash_attention_version \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   2255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda:\n",
      "\u001b[1;31mImportError\u001b[0m: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
     ]
    }
   ],
   "source": [
    "model_path = \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    _attn_implementation=\"flash_attention_2\"\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4034adb8",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1b86c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, a flattened version of your label tree:\n",
    "ALL_CLASSES = [\n",
    "    \"Cold Storage\", \"FairPrice\", \"Giant\", \"Ikea\", \"Mustafa\", \"Other Trolleys\", \"ShengSiong\",\n",
    "    \"Bird Issues\", \"Cat Issues\", \"Dead Animal\", \"Dog Issues\", \"Injured Animal\", \"Other Animal Issues\",\n",
    "    \"Bulky Waste in Common Areas\", \"Dirty Public Areas\", \"High-rise Littering\", \"Overflowing Litter Bin\",\n",
    "    \"Construction Noise\",\n",
    "    \"Choked Drain or Stagnant Water\", \"Damaged Drain\", \"Flooding\", \"Sewage Smell\", \"Sewer Choke or Overflow\",\n",
    "    \"No Water\", \"Water Leak\", \"Water Pressure\", \"Water Quality\",\n",
    "    \"Common Area Maintenance\", \"HDB Car Park Maintenance\", \"Lightning Maintenance\", \"Playground & Fitness Facilities Maintenance\",\n",
    "    \"HDB or URA Car Park\", \"Motorcycle at Void Deck\", \"Road\",\n",
    "    \"Others\",  # this is often used as a catch-all category\n",
    "    \"Fallen Tree or Branch\", \"Other Parks and Greenery Issues\", \"Overgrown Grass\", \"Park Facilities Maintenance\", \"Park Lighting Maintenance\",\n",
    "    \"Bee & Hornets\", \"Cockroaches in Food Establishment\", \"Mosquitoes\", \"Rodents in Common Areas\", \"Rodents in Food Establishment\",\n",
    "    \"Covered Linkway Maintenance\", \"Damaged Road Signs\", \"Faulty Streetlight\", \"Footpath Maintenance\", \"Road Maintenance\",\n",
    "    \"Anywheel\", \"HelloRide\", \"Other Bicycles\",\n",
    "    \"Food Premises\", \"Other Public Areas\", \"Parks & Park Connectors\"\n",
    "]\n",
    "\n",
    "label_to_idx = {label: idx for idx, label in enumerate(ALL_CLASSES)}\n",
    "NUM_CLASSES = len(ALL_CLASSES)\n",
    "\n",
    "data_root = \"./data/2. Sorted\"  # Adjust to your data folder path\n",
    "label_vector_file = \"./label_vectors.json\"  # Path to your label vector JSON file\n",
    "\n",
    "# System prompt for the model\n",
    "system_prompt = (\n",
    "    \"You are an expert in municipal services issues. Your task is to analyze the provided input, \"\n",
    "    \"which may include an image and a description, and categorize the issue into one or more categories \"\n",
    "    \"from the predefined list of municipal service issue types. Additionally, assess the severity of the issue \"\n",
    "    \"as one of the following: Low, Medium, or High.\\n\\n\"\n",
    "    \"The predefined list of categories is as follows:\\n\"\n",
    "    + \"\\n\".join(f\"- {category}\" for category in ALL_CLASSES) +\n",
    "    \"\\n\\nYour response should be in the following JSON format:\\n\"\n",
    "    \"{\\n\"\n",
    "    \"    \\\"categories\\\": [categories],\\n\"\n",
    "    \"    \\\"Severity\\\": severity\\n\"\n",
    "    \"}\\n\\n\"\n",
    "    \"Ensure that the categories are selected from the provided list of issue types, and the severity is determined \"\n",
    "    \"based on the details provided in the input.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95b2862",
   "metadata": {},
   "source": [
    "### Think have to redo this a bit\n",
    "Thought collate_fn in the finetuning example is the data loader. collate_fn which is kinda what i made here is the batching function. Data loader just returns the dataset raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc33a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a training dataset\n",
    "class FixMyStreetDataset(Dataset):\n",
    "    def __init__(self, data_root, label_vector_file):\n",
    "        \"\"\"\n",
    "        data_root: Folder containing the image and JSON files.\n",
    "        label_vector_file: Path to the JSON file mapping report IDs to label lists.\n",
    "        \"\"\"\n",
    "        self.data_root = data_root\n",
    "        \n",
    "        # Load label mapping (file names without extension as keys)\n",
    "        with open(label_vector_file, 'r', encoding='utf-8') as f:\n",
    "            self.label_vector = json.load(f)\n",
    "            \n",
    "        self.report_ids = list(self.label_vector.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.report_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        report_id = self.report_ids[idx]\n",
    "        \n",
    "        # Get label vector entry for the current report ID\n",
    "        label_entry = self.label_vector[report_id]\n",
    "\n",
    "        image_path = label_entry[\"image_path\"]\n",
    "\n",
    "        # Load the JSON metadata\n",
    "        relative_path = os.path.relpath(image_path, start=\"./data/2. Sorted\")  # Adjust to relative path\n",
    "        json_path = os.path.join(self.data_root, relative_path)\n",
    "        json_path = os.path.splitext(json_path)[0] + \".json\"  # Replace the image extension with .json\n",
    "        with open(json_path, 'r', encoding=\"utf-8\") as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        # Prepare the text content\n",
    "        text = metadata[\"description\"] + \"\\n\\n\"\n",
    "        text += \"Nearby location tags: \" + \", \".join([f\"{k}: {v}\" for tag in metadata[\"tags\"][\"nearby\"] for k, v in tag.items()]) + \"\\n\\n\"\n",
    "        text += \"Enclosing location tags: \" + \", \".join([f\"{k}: {v}\" for tag in metadata[\"tags\"][\"enclosing\"] for k, v in tag.items()])\n",
    "        \n",
    "        # Check if the image exists\n",
    "        if image_path:\n",
    "            # Load the image\n",
    "            image = Image.open(image_path)\n",
    "            if image.mode != \"RGB\":\n",
    "                image = image.convert(\"RGB\")\n",
    "        else:\n",
    "            # Placeholder for missing image\n",
    "            image = torch.zeros((3, 224, 224), dtype=torch.uint8)\n",
    "\n",
    "\n",
    "        return text, image\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d29b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching function\n",
    "def collate_fn(data):\n",
    "    print(data)\n",
    "    '''\n",
    "    user_content = [{\"type\": \"text\", \"text\": input_text}]\n",
    "\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": answer}\n",
    "        ]\n",
    "\n",
    "        # Process the inputs using the processor\n",
    "        text = processor.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8837d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "dataset = FixMyStreetDataset(data_root=data_root,\n",
    "                             label_vector_file=label_vector_file)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c9909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7117ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ds = dataset[\"validation\"].train_test_split(test_size=0.5)\n",
    "train_ds = split_ds[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205fb7bb",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf1083",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_path.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=250,\n",
    "    save_total_limit=1,\n",
    "    optim=\"paged_adamw_8bit\", # for 8-bit, keep this, else adamw_hf\n",
    "    bf16=True, # underlying precision for 8bit\n",
    "    output_dir=f\"./{model_name}-vqav2\",\n",
    "    hub_model_id=f\"{model_name}-vqav2\",\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee464ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=train_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b39a19",
   "metadata": {},
   "source": [
    "## COMPLETE LABEL BULLSHIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9576d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for the model\n",
    "system_prompt = (\n",
    "    \"You are an expert in municipal services issues. Your task is to analyze the provided input, \"\n",
    "    \"which may include an image and a description, and categorize the issue into one or more categories \"\n",
    "    \"from the predefined list of municipal service issue types. Additionally, assess the severity of the issue \"\n",
    "    \"as one of the following: Low, Medium, or High.\\n\\n\"\n",
    "    \"The predefined list of categories is as follows:\\n\"\n",
    "    + \"\\n\".join(f\"- {category}\" for category in ALL_CLASSES) +\n",
    "    \"\\n\\nYour response should be in the following JSON format:\\n\"\n",
    "    \"{\\n\"\n",
    "    \"    \\\"categories\\\": [categories],\\n\"\n",
    "    \"    \\\"Severity\\\": severity\\n\"\n",
    "    \"}\\n\\n\"\n",
    "    \"Ensure that the categories are selected from the provided list of issue types, and the severity is determined \"\n",
    "    \"based on the details provided in the input.\"\n",
    ")\n",
    "\n",
    "data_root = \"./data/2. Sorted\"\n",
    "label_vector_file = \"./label_vectors.json\"\n",
    "\n",
    "# iterate through label_vector_file and get the image path and json path\n",
    "with open(label_vector_file, 'r', encoding='utf-8') as f:\n",
    "    label_vector = json.load(f)\n",
    "\n",
    "    for report_id, label_entry in label_vector.items():\n",
    "        # Load the JSON metadata\n",
    "        # get path by accessing image_path and replacing extension with .json\n",
    "        json_path = os.path.splitext(label_entry[\"image_path\"])[0] + \".json\"\n",
    "        with open(json_path, 'r', encoding=\"utf-8\") as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        # Prepare the text content for the processor\n",
    "        input_text = metadata[\"description\"] + \"\\n\\n\"\n",
    "        input_text += \"Nearby location tags: \" + \", \".join([f\"{k}: {v}\" for tag in metadata[\"tags\"][\"nearby\"] for k, v in tag.items()]) + \"\\n\\n\"\n",
    "        input_text += \"Enclosing location tags: \" + \", \".join([f\"{k}: {v}\" for tag in metadata[\"tags\"][\"enclosing\"] for k, v in tag.items()])\n",
    "        user_content = [{\"type\": \"text\", \"text\": input_text}]\n",
    "\n",
    "        # Load the image\n",
    "        image_path = label_entry[\"image_path\"]\n",
    "        if image_path:\n",
    "            image = Image.open(image_path)\n",
    "            if image.mode != \"RGB\":\n",
    "                image = image.convert(\"RGB\")\n",
    "        else:\n",
    "            image = torch.zeros((3, 224, 224), dtype=torch.uint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a80b710",
   "metadata": {},
   "source": [
    "## Qwen2.5VM stuff (dont touch for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a611d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title inference function\n",
    "def inference(image_path, prompt, sys_prompt=\"You are a helpful assistant.\", max_new_tokens=4096, return_input=False):\n",
    "    image = Image.open(image_path)\n",
    "    image_local_path = \"file://\" + image_path\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"image\": image_local_path},\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    print(\"text:\", text)\n",
    "    # image_inputs, video_inputs = process_vision_info([messages])\n",
    "    inputs = processor(text=[text], images=[image], padding=True, return_tensors=\"pt\")\n",
    "    inputs = inputs.to('cuda')\n",
    "\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    if return_input:\n",
    "        return output_text[0], inputs\n",
    "    else:\n",
    "        return output_text[0]\n",
    "    \n",
    "#  base 64 编码格式\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a907dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(model_path, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\",device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aef48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
