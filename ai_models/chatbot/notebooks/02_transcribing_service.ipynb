{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c6f96a",
   "metadata": {},
   "source": [
    "# **2.0** ‎ Transcribing Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deabd4d",
   "metadata": {},
   "source": [
    "This notebook serves as a hands-on trial and evaluation environment for various **Speech-to-Text (STT)** services and models. \\\n",
    "The goal is to identify a suitable transcription solution for use within our municipal chatbot pipeline. \\\n",
    "**NOTE**: Due to time limitations for now, we'll only be exploring Whisper, a state-of-the-art model for automatic speech recognition (ASR)\n",
    "\n",
    "A brief summary of what we aim to achieve here is to:\n",
    "- Compare multiple STT services or models in terms of **accuracy**, **latency**, **speaker handling**, and **language support**\n",
    "\n",
    "- Identify the trade-offs between **open-source** vs **API-based** STT systems\n",
    "\n",
    "- Pre-process audio if required (e.g., format conversion, downsampling)\n",
    "\n",
    "- Optionally, demonstrate a **Text-to-Speech (TTS)** service for response generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce060d",
   "metadata": {},
   "source": [
    "### **2.0.1** ‎ ‎ Install Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf7e31b",
   "metadata": {},
   "source": [
    "Many transcribing services uses ffmpeg under the hood. You’ll need ffmpeg installed on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4246a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ffmpeg-python --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a85317",
   "metadata": {},
   "source": [
    "Depending on which OS you're on, you can run the following command in your terminal to install it:\n",
    "- **macOS**: `brew install ffmpeg`\n",
    "\n",
    "- **Ubuntu (Linux)**: `sudo apt install ffmpeg`\n",
    "\n",
    "- **Windows**: `choco install ffmpeg` or you may manually install it here: https://www.ffmpeg.org/download.html\n",
    "\n",
    "You can check whether ffmpeg has been successfully installed by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08069b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.1.1-essentials_build-www.gyan.dev Copyright (c) 2000-2025 the FFmpeg developers\n",
      "built with gcc 14.2.0 (Rev1, Built by MSYS2 project)\n",
      "configuration: --enable-gpl --enable-version3 --enable-static --disable-w32threads --disable-autodetect --enable-fontconfig --enable-iconv --enable-gnutls --enable-libxml2 --enable-gmp --enable-bzlib --enable-lzma --enable-zlib --enable-libsrt --enable-libssh --enable-libzmq --enable-avisynth --enable-sdl2 --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxvid --enable-libaom --enable-libopenjpeg --enable-libvpx --enable-mediafoundation --enable-libass --enable-libfreetype --enable-libfribidi --enable-libharfbuzz --enable-libvidstab --enable-libvmaf --enable-libzimg --enable-amf --enable-cuda-llvm --enable-cuvid --enable-dxva2 --enable-d3d11va --enable-d3d12va --enable-ffnvcodec --enable-libvpl --enable-nvdec --enable-nvenc --enable-vaapi --enable-libgme --enable-libopenmpt --enable-libopencore-amrwb --enable-libmp3lame --enable-libtheora --enable-libvo-amrwbenc --enable-libgsm --enable-libopencore-amrnb --enable-libopus --enable-libspeex --enable-libvorbis --enable-librubberband\n",
      "libavutil      59. 39.100 / 59. 39.100\n",
      "libavcodec     61. 19.101 / 61. 19.101\n",
      "libavformat    61.  7.100 / 61.  7.100\n",
      "libavdevice    61.  3.100 / 61.  3.100\n",
      "libavfilter    10.  4.100 / 10.  4.100\n",
      "libswscale      8.  3.100 /  8.  3.100\n",
      "libswresample   5.  3.100 /  5.  3.100\n",
      "libpostproc    58.  3.100 / 58.  3.100\n"
     ]
    }
   ],
   "source": [
    "!ffmpeg -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5673eddd",
   "metadata": {},
   "source": [
    "### **2.0.2** ‎ ‎ Load or Record Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5e1aa4",
   "metadata": {},
   "source": [
    "You can choose between two input methods in this notebook:\n",
    "1. Upload or reference an audio file (WAV/MP3)\n",
    "\n",
    "2. Record directly from microphone (ipynb or CLI)\n",
    "\n",
    "If you like to record the audio from your microphone it, you can run the below cell, and it'll generate an audio file (`.wav` or `.mp3`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32be646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording saved as mic_input.wav\n"
     ]
    }
   ],
   "source": [
    "# Record from microphone (may not work in hosted notebooks)\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "def record_audio(filename=\"../data/assets/input/mic_input_01.wav\", duration=5, samplerate=16000):\n",
    "    print(\"Recording...\")\n",
    "    recording = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1)\n",
    "    sd.wait()\n",
    "    write(filename, samplerate, recording)\n",
    "    print(\"Recording saved as\", filename)\n",
    "\n",
    "# Record for 5 seconds\n",
    "record_audio(duration=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad38f2b",
   "metadata": {},
   "source": [
    "### **2.0.3** ‎ ‎ Audio Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70f71e8",
   "metadata": {},
   "source": [
    "Before passing audio files into a STT model, it is important to ensure that they meet the model's expected input format. \\\n",
    "Preprocessing helps improve transcription accuracy and prevent inference errors.\n",
    "The following table shows the common preprocessing procedures involved when it comes to STT:\n",
    "\n",
    "| Step         | Purpose                                                     |\n",
    "|------------------|-----------------------------------------------------------------|\n",
    "| **Resampling**     | Ensures consistent sample rate (e.g. Whisper expects 16 kHz audio)                      |\n",
    "| **Mono Conversion**      | STT models expect a single audio channel                                 |\n",
    "| **Volume Normalisation**   | Prevents clipping and ensures even loudness                     |\n",
    "| **Silence Trimming** | Reduces irrelevant audio and improves accuracy            |\n",
    "| **File Format Conversion** | Converts to `.wav` (PCM) or other supported formats     |\n",
    "\n",
    "However, since for now, all audio input will be generated from our side, this step is very unlikely needed. \\\n",
    "But if we ever do need it, we'll need to first install these dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83b8b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydub torchaudio librosa --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e71b0b",
   "metadata": {},
   "source": [
    "Then, run the below function with your input file, and it will follow the steps outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df6f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import torchaudio\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def preprocess_audio(input_path, output_path=\"../data/assets/input/mic_input_preprocessed.wav\", target_sample_rate=16000, trim_silence=True):\n",
    "    \"\"\"\n",
    "    Preprocess an audio file:\n",
    "    - Converts to mono\n",
    "    - Resamples to 16 kHz\n",
    "    - Trims silence (optional)\n",
    "    - Saves as 16-bit PCM WAV\n",
    "    \"\"\"\n",
    "    # Load audio\n",
    "    audio = AudioSegment.from_file(input_path)\n",
    "\n",
    "    # Convert to mono\n",
    "    audio = audio.set_channels(1)\n",
    "\n",
    "    # Resample to 16 kHz\n",
    "    audio = audio.set_frame_rate(target_sample_rate)\n",
    "\n",
    "    # Normalize volume\n",
    "    audio = audio.apply_gain(-audio.max_dBFS)\n",
    "\n",
    "    # Trim silence from beginning and end\n",
    "    if trim_silence:\n",
    "        # Convert to raw samples for trimming\n",
    "        samples = audio.get_array_of_samples()\n",
    "        waveform = torch.tensor(samples, dtype=torch.float32).unsqueeze(0)\n",
    "        trimmed_waveform, _ = torchaudio.transforms.Vad(sample_rate=target_sample_rate)(waveform)\n",
    "        torchaudio.save(output_path, trimmed_waveform, target_sample_rate)\n",
    "    else:\n",
    "        # Export directly as WAV\n",
    "        audio.export(output_path, format=\"wav\", parameters=[\"-ar\", str(target_sample_rate), \"-ac\", \"1\"])\n",
    "\n",
    "    print(f\"Preprocessed audio saved to: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "preprocessed_path = preprocess_audio(\"../data/assets/input/mic_input_01.wav\", output_path=\"../data/assets/input/mic_input_preprocessed_01.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe7824d",
   "metadata": {},
   "source": [
    "# **2.1** ‎ Implementing Speech-to-Text (STT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ea7ca",
   "metadata": {},
   "source": [
    "### Why STT Matters for a Municipal Chatbot in Singapore?\n",
    "\n",
    "For a conversational assistant targeting municipal services in Singapore, STT plays a **critical role in accessibility and multi-modal interaction**:\n",
    "\n",
    "- **Voice Input Support**: Residents can speak instead of type, which is faster and more natural for many users.\n",
    "\n",
    "- **Multilingual Use Cases**: STT enables understanding across languages commonly spoken in Singapore (e.g., English, Chinese, Malay, Tamil).\n",
    "\n",
    "- **On-the-Go Reporting**: Users can report potholes or noise complaints hands-free while on the move.\n",
    "\n",
    "- **Inclusive Design**: Makes the system more usable for elderly or less tech-savvy citizens.\n",
    "\n",
    "A robust STT module allows the chatbot to transcribe and understand citizen-reported issues with high accuracy, even in noisy outdoor environments. \\\n",
    "It also lays the groundwork for future **voice-to-voice** interaction, where the system can both listen and respond with speech. \\\n",
    "We will evaluate both open-source and cloud-based STT solutions to find a balance between performance, privacy, and deployment feasibility in a Singaporean context.\n",
    "\n",
    "### Evaluation Criteria\n",
    "| Criteria         | Description                                                     |\n",
    "|------------------|-----------------------------------------------------------------|\n",
    "| **Accuracy**     | Word Error Rate (WER) or perceived quality                      |\n",
    "| **Latency**      | Time taken to transcribe input                                  |\n",
    "| **Robustness**   | Handles different accents, noise, or speeds                     |\n",
    "| **Local vs Cloud** | Trade-offs in privacy, cost, setup, and ease of use            |\n",
    "| **Multi-language** | Whether models can handle code-switching or regional terms     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3c188f",
   "metadata": {},
   "source": [
    "### **2.1.1** ‎ ‎ OpenAI Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb83378d",
   "metadata": {},
   "source": [
    "**Whisper** is an open-source ASR system developed by OpenAI, trained on 680,000 hours of multilingual and multitask supervised data collected from the web. \\\n",
    "It is designed to be **robust, general-purpose, and highly accurate** across a wide range of accents, background noise levels, and speech patterns.\n",
    "\n",
    "Below are some reasons why we believe Whisper can be an excellent choice for our solution:\n",
    "- **Multilingual**: Supports over 90 languages, including English, Chinese, Malay, and Tamil – making it ideal for use in multilingual environments like SG.\n",
    "\n",
    "- **Punctuation and Formatting**: Automatically adds punctuation and casing for readable output for the chatbot.\n",
    "\n",
    "- **Robust to Noise**: Performs well in real-world, noisy conditions (e.g., urban street recordings).\n",
    "\n",
    "- **Language Identification**: Automatically detects the spoken language.\n",
    "\n",
    "- **Open Source**: Fully available via GitHub (https://github.com/openai/whisper), with multiple model sizes (tiny, large etc.).\n",
    "\n",
    "There needs to be a good balance between speed and it's abilty to handle a good amount of scenarios, so choosing the right size is important.  \n",
    "Below is an general estimation comparison between the model variants Whisper provides:\n",
    "| Variant     | Size     | Speed     | Accuracy       |\n",
    "|-------------|----------|-----------|----------------|\n",
    "| `tiny`      | ~39 MB   | Fast  | Basic          | \n",
    "| `base`      | ~74 MB   | Fast   | Fair           | \n",
    "| `small`     | ~244 MB  | Medium  | Good           | \n",
    "| `medium`    | ~769 MB  | Slower | Very Good      | \n",
    "| `large`     | ~1.55 GB | Slow    | Best  |\n",
    "\n",
    "We use the `openai-whisper` Python package to load a pre-trained Whisper model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271dd6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai-whisper --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1264fb20",
   "metadata": {},
   "source": [
    "**Load Whisper Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39b39d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [00:09<00:00, 15.7MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "# Load tiny model\n",
    "model = whisper.load_model(\"tiny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753a2ab",
   "metadata": {},
   "source": [
    "**Transcribing with Whisper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c361a981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript:  What is NEA responsible for?\n"
     ]
    }
   ],
   "source": [
    "def transcribe_audio(audio_path):\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "# Example usage\n",
    "text = transcribe_audio(\"../data/assets/input/mic_input_preprocessed_01.wav.wav\")\n",
    "print(\"Transcript:\", text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
