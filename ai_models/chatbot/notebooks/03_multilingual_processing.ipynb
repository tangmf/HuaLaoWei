{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "203e23d6",
   "metadata": {},
   "source": [
    "# **3.0** ‎ Multilingual Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bf4ebb",
   "metadata": {},
   "source": [
    "This notebook focuses on designing and evaluating a **lightweight multilingual processing pipeline**. \\\n",
    "It is to enable the municipal chatbot to handle input in multiple languages commonly used in Singapore (English, Chinese, Malay & Tamil). \\\n",
    "We will explore two approaches throughout this notebook:\n",
    "\n",
    "1. **Language Detection + Translation to English**\n",
    "\n",
    "2. **Multilingual LLMs that natively understand input in different languages**\n",
    "\n",
    "Based on our testings and discoveries, our objective is to find a solution that can offer:\n",
    "- The ability to understand inputs in English, Mandarin, Malay, & Tamil and respond appropriately regardless of input language\n",
    "\n",
    "- The abiliy to handle code-switched or colloquial speech (e.g., Singlish) \n",
    "\n",
    "- A balance between **accuracy**, **speed**, and **resource efficiency**—making it suitable for real-time uses "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06cdd59",
   "metadata": {},
   "source": [
    "# **3.1** ‎ Language Detection + Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0c48e7",
   "metadata": {},
   "source": [
    "This approach involves two stages:\n",
    "1. **Detect the language** of the incoming text using a lightweight model.\n",
    "\n",
    "2. **Translate** the input into English using an automatic translation model.\n",
    "\n",
    "Once translated, the query can be routed to an English-only intent classifier or chatbot logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c81694",
   "metadata": {},
   "source": [
    "### **3.1a.1** ‎ *Detection* – langid.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab07bf7f",
   "metadata": {},
   "source": [
    "[`langid`](https://github.com/saffsd/langid.py) is a lightweight, offline language identification library trained on over 97 languages. \\\n",
    "It uses a Naive Bayes classifier over character n-grams, making it compact, fast, and suitable for embedded or low-resource environments.\n",
    "\n",
    "Why `langid` Might Be a Good Choice:\n",
    "- **Offline and Self-Contained**: Requires no internet connection or external models.\n",
    "- **Fast Inference**: Performs language detection in milliseconds, ideal for real-time applications.\n",
    "- **Small Footprint**: No large dependencies; easy to integrate into lightweight deployments.\n",
    "- **Good Accuracy on Short Texts**: Designed to handle short snippets like search queries and tweets—similar in length to typical chatbot inputs.\n",
    "- **Stable and Mature**: Despite its simplicity, `langid` has been widely used in production systems.\n",
    "- **Language Support**: Offers strong support for the 4 main languages in Singapore.\n",
    "\n",
    "Noticeable Limitations of `langid`:\n",
    "- May not recognize **code-switched** text (e.g., mixed English-Chinese or Singlish).\n",
    "- Limited support for **regional variations or dialects** (e.g., Tamil variants used in Singapore).\n",
    "- Trained on formal text corpora—accuracy may drop for informal or slang-heavy messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7743e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langid --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3f70fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import langid\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "# Load NLLB distilled model\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "translator_pipeline = pipeline(\"translation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcdd10a",
   "metadata": {},
   "source": [
    "### **3.1b.1** ‎ *Detection* – Meta AI's (Facebook) NLLB "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2eea30",
   "metadata": {},
   "source": [
    "**NLLB** (No Language Left Behind) supports high-quality machine translation across 200+ languages, inclusive of many underrepresented and and low-resource languages. \\\n",
    "The NLLB-200 Distilled 600M model is a compressed, faster version optimised for inference speed while retaining strong translation quality.\n",
    "\n",
    "**Why NLLB-200 Distilled 600M Is a Strong Candidate**\n",
    "\n",
    "- **Supports 200+ Languages**: Includes Singapore's major languages—English, Chinese (`zho_Hans`), Malay (`zsm_Latn`), and Tamil (`tam_Taml`).\n",
    "- **High Translation Quality**: Benchmarked against FLORES-200, with competitive performance in many low-resource languages.\n",
    "- **Distilled for Speed**: The 600M version balances translation quality with faster inference and lower memory usage.\n",
    "- **Open Source**: Available via Hugging Face (`facebook/nllb-200-distilled-600M`) with permissive licensing for integration.\n",
    "\n",
    "However, some consideration and drawbacks of it to take note off:\n",
    "- Requires specifying the correct source and target language codes (other larger size versions of the model has it in-built)\n",
    "- May struggle with informal phrases or code-switching\n",
    "- Requires the use of a language detection system first before it can translate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af195d2d",
   "metadata": {},
   "source": [
    "**Install Depdencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331e4a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8ef87a",
   "metadata": {},
   "source": [
    "**Define Supported Language Mapping**\n",
    "\n",
    "As mentioned, we have to manually map language codes from `langid` (ISO 639-1) to NLLB's internal codes. For now, we support:\n",
    "\n",
    "- English ➠ `eng_Latn` (no translation needed)\n",
    "- Chinese ➠ `zho_Hans`\n",
    "- Malay ➠ `msa_Latn`\n",
    "- Tamil ➠ `tam_Taml`\n",
    "\n",
    "Any other languages will be marked as unsupported for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0829716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supported languages\n",
    "lang_map = {\n",
    "    \"en\": \"eng_Latn\",\n",
    "    \"zh\": \"zho_Hans\",\n",
    "    \"ms\": \"msa_Latn\",\n",
    "    \"ta\": \"tam_Taml\"\n",
    "}\n",
    "\n",
    "def detect_language(text):\n",
    "    lang, prob = langid.classify(text)\n",
    "    return lang, prob\n",
    "\n",
    "def translate_to_english(text):\n",
    "    lang, prob = detect_language(text)\n",
    "    \n",
    "    if lang == \"en\":\n",
    "        return text, \"eng_Latn\", False  # No translation needed\n",
    "\n",
    "    if lang not in lang_map:\n",
    "        return text, lang, False  # Unsupported language\n",
    "\n",
    "    src_lang = lang_map[lang]\n",
    "    translation = translator_pipeline(text, src_lang=src_lang, tgt_lang=\"eng_Latn\")\n",
    "    return translation[0]['translation_text'], src_lang, True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01de606",
   "metadata": {},
   "source": [
    "**Test the Translation Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c66004",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"这个垃圾桶已经满了\",  # Chinese\n",
    "    \"Di mana saya boleh laporkan kebersihan kawasan awam?\",  # Malay\n",
    "    \"நாங்கள் எங்கு புகார் அளிக்கலாம்?\",  # Tamil\n",
    "    \"Where do I report illegal dumping?\"  # English\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    translated, src_lang, was_translated = translate_to_english(q)\n",
    "    print(\"Original:\", q)\n",
    "    print(\"Detected Language:\", src_lang)\n",
    "    print(\"Was Translated:\", was_translated)\n",
    "    print(\"Final Query (English):\", translated)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7eaf34",
   "metadata": {},
   "source": [
    "# **3.2** ‎ Multilingual LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca80a1",
   "metadata": {},
   "source": [
    "This approach uses LLMs trained to natively understand and reason across multiple languages. \\\n",
    "These models can directly process inputs in Mandarin, Malay, Tamil, and more—without needing translation.\n",
    "With this section, it will help us evaluate if:\n",
    "- LLMs already understand these languages well enough?\n",
    "- Is translation necessary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e18292f",
   "metadata": {},
   "source": [
    "### **3.2.1** ‎ Deepseek Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c8cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"deepseek-r1:7b\")\n",
    "\n",
    "def query_llm_directly(prompt):\n",
    "    return llm.invoke(prompt).strip()\n",
    "\n",
    "for q in test_queries:\n",
    "    prompt = f\"You are a helpful municipal assistant. The user said:\\n\\n{q}\\n\\nRespond helpfully.\"\n",
    "    print(f\"Input: {q}\")\n",
    "    print(\"LLM Response:\")\n",
    "    print(query_llm_directly(prompt))\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364c6ba8",
   "metadata": {},
   "source": [
    "### **3.2.2** ‎ OpenAI ChatGPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f02f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e505c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"\",\n",
    ")\n",
    "def gpt4_response(query):\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        instructions=\"You are a helpful municipal assistant, that answers user's queries clear and consisely.\",\n",
    "        input=query,\n",
    "    )\n",
    "    return response.output_text.strip()\n",
    "\n",
    "for q in test_queries:\n",
    "    prompt = f\"{q}\"\n",
    "    print(f\"Input: {q}\")\n",
    "    print(\"LLM Response:\")\n",
    "    print(gpt4_response(prompt))\n",
    "    print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
