{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f58706",
   "metadata": {},
   "source": [
    "# **1.0** ‎ Installation & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4690ed",
   "metadata": {},
   "source": [
    "This notebook serves as the foundational setup guide for using **Ollama** to support the development of HuaLaoWei's municipal chatbot assistant. \\\n",
    "It focuses on installing Ollama, pulling suitable Large Language Models (LLMs), and validating their capabilities through quick benchmarking tests. \\\n",
    "The selected models will later be used for tasks such as intent classification, query filtering, and response generation in subsequent notebooks.\n",
    "\n",
    "### **Introduction:** What is Ollama?\n",
    "Ollama is a local Large Language Model (LLM) runtime that simplifies the process of downloading, running, and interacting with open-source language models on your own machine. Unlike cloud-hosted services (such as OpenAI or Cohere), Ollama provides privacy, cost-efficiency, and lower latency, making it ideal for local experimentation and development.\n",
    "\n",
    "### **Why Ollama?**\n",
    "- **Offline Access:** No internet dependency after download.\n",
    "- **Custom Model Support:** Easy to pull or fine-tune models.\n",
    "- **Reduced Latency:** Instant response generation without network delays.\n",
    "- **Privacy-first:** Sensitive queries remain local.\n",
    "\n",
    "We will use Ollama to run lightweight and domain-tuned LLMs for intent classification and other downstream tasks in our chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9e52f0",
   "metadata": {},
   "source": [
    "### **1.0.1** ‎ Installing Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c42fa63",
   "metadata": {},
   "source": [
    "To begin, if you're a macOS or Linux user, you can run the following command in your terminal, or in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2912c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1260dd",
   "metadata": {},
   "source": [
    "Then restart your shell or run the below command if you're in a notebook environment (Linux)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1884d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PATH=$PATH:/usr/local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e181aa75",
   "metadata": {},
   "source": [
    "On Windows, the downloader must be directly downloaded from: https://ollama.com/download \\\n",
    "More detailed installation instructions is available at: https://ollama.com \\\n",
    "To check if Ollama has been successfully installed, you can run the below command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52f4485a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama version is 0.6.5\n"
     ]
    }
   ],
   "source": [
    "# Check Ollama installation version\n",
    "!ollama --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25ab6fc",
   "metadata": {},
   "source": [
    "### **1.0.2** ‎ Serving Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd136c5",
   "metadata": {},
   "source": [
    "`ollama serve` starts the local Ollama backend service that handles model requests. \\\n",
    "It opens up a local HTTP API (usually on localhost:11434) that tools, scripts, or apps can connect to in order to run prompts, chat, etc. \\\n",
    "As we'll be using LangChain later on, this command helps connect Ollama connect to these services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d8a0ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.\n"
     ]
    }
   ],
   "source": [
    "!ollama serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa11ac9",
   "metadata": {},
   "source": [
    "### **1.0.3** ‎ Pull Pre-trained LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf7dc43",
   "metadata": {},
   "source": [
    "As mentioned, Ollama has a large collection of open-source pre-trained LLM models at our disposal. \\\n",
    "You can view their whole collection on their website and documentation here: https://ollama.com. \\\n",
    "For initial testing and experimentation, we will pull several of them. \\\n",
    "Below is a table that provides a general summary of the models we're pulling:\n",
    "\n",
    "| Model          | Size          | Speed (on CPU)       | Accuracy / Quality             | Use Cases                                         |\n",
    "|----------------|---------------|----------------------|--------------------------------|--------------------------------------------------------|\n",
    "| llama3.1     | ~8B / ~70B    | Moderate             | Very High (on 70B)             | General-purpose chat, reasoning, classification, RAG     |\n",
    "| mistral     | 7B            | Fast                 | Quite High (for its size)      | Lightweight tasks, few-shot classification, code help    |\n",
    "| chatglm3    | 6B            | Moderate             | Moderate–High (Bilingual)      | Bilingual bots, Q&A, regional language applications       |\n",
    "| deepseek-r1 | ~7B / ~70B    | Moderate             | Very High (GPT-3.5 class)      | Classification, summarisation, intent-to-code systems    |\n",
    "\n",
    "Use the following code below to download these models. \\\n",
    "**Note:** Initial installation for each model could take up to several minutes depending on your RAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb08c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama3.1 (Meta)\n",
    "!ollama pull llama3.1:8b\n",
    "\n",
    "# Mistral\n",
    "!ollama pull mistral\n",
    "\n",
    "# ChatGLM\n",
    "!ollama pull EntropyYue/chatglm3\n",
    "\n",
    "# Deepseek Chat\n",
    "!ollama pull deepseek-r1:7b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122aeab2",
   "metadata": {},
   "source": [
    "Vice versa, if you need to remove any models you've pulled, you can do that using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bb748d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted 'deepseek-coder:6.7b-instruct'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama rm <model_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0309668d",
   "metadata": {},
   "source": [
    "To check if Ollama is running and that the models has been successfully installed, you can execute the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c99b9222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running. Installed models:\n",
      "NAME                          ID              SIZE      MODIFIED           \n",
      "llama3.1:latest               46e0c10c039e    4.9 GB    About a minute ago    \n",
      "mistral:latest                f974a74358d6    4.1 GB    13 hours ago          \n",
      "EntropyYue/chatglm3:latest    8f6f34227356    3.6 GB    13 hours ago          \n",
      "deepseek-r1:7b                0a8c26691023    4.7 GB    3 days ago            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def check_ollama_status():\n",
    "    try:\n",
    "        output = subprocess.check_output([\"ollama\", \"list\"], stderr=subprocess.STDOUT, text=True)\n",
    "        print(\"Ollama is running. Installed models:\")\n",
    "        print(output)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error with Ollama:\", e.utput)\n",
    "\n",
    "check_ollama_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69f17e0",
   "metadata": {},
   "source": [
    "# **1.1** ‎ Testing Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e136f39",
   "metadata": {},
   "source": [
    "To better understand the performance and suitability of each pulled model for our municipal chatbot, we will run a small set of test prompts across all models. \\\n",
    "These tests can help us evaluate in terms of:\n",
    "\n",
    "- **Inference Speed**: Time taken to generate a full response.\n",
    "- **Output Quality**: Fluency, relevance, and structure of the response.\n",
    "- **Consistency**: How reliably the model answers similar queries.\n",
    "- **Suitability**: Whether the model aligns with municipal use cases.\n",
    "\n",
    "This benchmarking helps us identify the right model for different components of our chatbot architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26b8f271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing model: llama3.1:latest\n",
      "Inference Time: 18.26 sec\n",
      "Response:\n",
      "AI can enhance the municipal reporting service in several ways:\n",
      "\n",
      "1. **Automated data collection**: AI can extract relevant information from various sources, such as social media, news articles, and government websites, reducing manual labor and increasing accuracy.\n",
      "2. **Predictive analytics**: AI-powered algorithms can analyze historical data to predict future trends, enabling proactive decision-making and resource allocation.\n",
      "3. **Improved incident classification**: AI can categorize and prioritize reports based on urgency and severity, ensuring timely attention from municipal authorities.\n",
      "4. **Enhanced citizen engagement**: AI-driven chatbots or virtual assistants can provide citizens with information, updates, and guidance on reporting processes, fostering a more user-friendly experience.\n",
      "5. **Real-time monitoring**: AI can analyze data streams in real-time to detect anomalies, enabling swift response to emerging issues.\n",
      "\n",
      "By leveraging these capabilities, municipalities can streamline their reporting services, improve efficiency, and enhance citizen satisfaction.\n",
      "\n",
      "Testing model: mistral:latest\n",
      "Inference Time: 13.96 sec\n",
      "Response:\n",
      " AI can significantly improve municipal reporting services by implementing the following ways:\n",
      "\n",
      "1. Chatbots: AI-powered chatbots can be integrated into the city's website or mobile app, making it easier for citizens to report issues such as potholes, damaged street signs, or litter in real time. These bots can understand and interpret natural language, simplifying the process for users.\n",
      "\n",
      "2. Predictive Analytics: AI algorithms can analyze patterns and trends from historical data to predict potential problems before they occur. For example, by analyzing pothole repair data, AI can forecast where potholes are most likely to form and alert maintenance crews to address the issue proactively.\n",
      "\n",
      "3. Automated Response Systems: AI can process and categorize incoming reports quickly, ensuring that they reach the appropriate department for resolution. This helps city officials prioritize issues based on severity and location.\n",
      "\n",
      "4. Sentiment Analysis: By analyzing social media posts and feedback forms, AI can help municipalities gauge citizen satisfaction with services and identify areas of improvement.\n",
      "\n",
      "5. Smart City Infrastructure: Integrating sensors, cameras, and IoT devices throughout the city can provide real-time data on traffic congestion, air quality, waste management, and more. AI can analyze this data to optimize resource allocation and improve service delivery.\n",
      "\n",
      "6. Virtual Reality (VR) Inspections: AI combined with VR technology enables remote inspections of public facilities, reducing the need for physical site visits and improving safety during emergencies or pandemics.\n",
      "\n",
      "Testing model: EntropyYue/chatglm3:latest\n",
      "Inference Time: 8.51 sec\n",
      "Response:\n",
      " AI can be leveraged to automate and streamline the municipal reporting process, making it more efficient and cost-effective for municipalities. By using natural language processing (NLP) and machine learning (ML) techniques, municipal reports can be analyzed and processed faster, leading to shorter turnaround times for information release. Additionally, AI can help identify patterns and insights within the data that would otherwise go unnoticed by human analysts, providing valuable insights that can inform decision-making processes.\n",
      "\n",
      "Testing model: deepseek-r1:7b\n",
      "Inference Time: 53.25 sec\n",
      "Response:\n",
      "<think>\n",
      "Okay, so I need to figure out how AI can help improve municipal reporting services. Hmm, where do I start?\n",
      "\n",
      "First off, I know that municipalities handle a lot of reports from citizens about things like street repairs,公园 maintenance, trash collection, etc. These reports are probably sent through traditional methods like letters or phone calls, which can be slow and inefficient.\n",
      "\n",
      "Maybe AI can make this process faster. I've heard about chatbots being used in customer service, so perhaps an AI chatbot could answer common questions or route less urgent issues to human staff. That sounds efficient.\n",
      "\n",
      "Wait, but how does that help? Well, it would reduce the number of calls or letters that need to be processed by humans, freeing them up for more complex tasks. Also, AI can analyze data patterns. For example, if certain streets always get reports of potholes on a specific date due to weather, the AI could predict when they'll need fixing and schedule accordingly.\n",
      "\n",
      "Another idea is sentiment analysis. People write reports with different tones—some frustrated, others positive. Using this, municipalities might understand public mood better or identify recurring issues before they escalate. That's pretty cool because it gives proactive insights.\n",
      "\n",
      "Then there's automation for data entry. If the AI can take scanned forms and convert them into digital records, that would save time compared to manual entry. Plus, OCR technology is already out there for reading text from images, so integrating that could help with processing reports more efficiently.\n",
      "\n",
      "Predictive maintenance comes to mind too. Using historical data on infrastructure like roads or buildings, AI could predict when maintenance might be needed based on usage patterns and weather forecasts. This proactive approach can prevent bigger problems down the line.\n",
      "\n",
      "Personalization is another angle. If an AI learns a user's history of issues, it can provide tailored recommendations for their specific problems. For instance, if someone frequently reports expiredpermits, the AI could suggest reminders or automated follow-ups to help them stay in compliance without constant oversight.\n",
      "\n",
      "Integration with other systems might streamline operations. Maybe connecting AI tools with city databases for population stats or traffic data so that reports can be analyzed holistically. This interconnected approach could offer deeper insights and better decision-making support.\n",
      "\n",
      "Wait, but how exactly does the AI do all this? It must process a lot of data, probably from various sources like surveys, historical reports, etc. Machine learning models trained on this data would help in predictions and classifications. Also, natural language processing allows the AI to understand and respond to different types of queries or comments.\n",
      "\n",
      "I should also consider user feedback loops. As people interact with the AI system, it can learn from their experiences, improving over time. This continuous improvement could make the reporting process even better suited to users' needs.\n",
      "\n",
      "But are there challenges? Data privacy is a big one. Municipalities have access to sensitive information about citizens, so ensuring that all data handling complies with regulations like GDPR or CCPA is crucial. Plus, integrating AI into existing systems might require significant infrastructure investment and training for staff.\n",
      "\n",
      "Overall, the potential benefits seem substantial: faster response times, better resource allocation, proactive issue management, improved user experience through tailored support, and enhanced decision-making based on real-time data analysis. It's clear that AI has a lot to offer in making municipal services more efficient and responsive.\n",
      "</think>\n",
      "\n",
      "AI can significantly enhance municipal reporting services by:\n",
      "\n",
      "1. **Streamlining Communication**: Implementing AI chatbots to address common questions or routing urgent issues to human staff, reducing the volume of manual inquiries.\n",
      "\n",
      "2. **Predictive Analytics**: Utilizing AI to analyze historical data patterns, such as recurring issues related to weather, and predicting when maintenance might be needed, allowing proactive scheduling.\n",
      "\n",
      "3. **Sentiment Analysis**: Employing natural language processing to understand public sentiment from reports, providing insights into recurring problems or areas for improvement without requiring manual data entry.\n",
      "\n",
      "4. **Efficient Data Handling**: Automating processes like converting scanned forms into digital records using OCR technology and machine learning models to predict future issues based on historical data.\n",
      "\n",
      "5. **Personalized Support**: Tailoring recommendations to individual users by analyzing their history, such as suggesting reminders or follow-ups for specific issues like expired permits.\n",
      "\n",
      "6. **Integrated Systems**: Connecting AI tools with city databases and other systems to offer a holistic view of operations, enabling better insights and decision-making through comprehensive data analysis.\n",
      "\n",
      "7. **Continuous Improvement**: Leveraging user feedback loops to continuously improve the system, ensuring it meets evolving needs and preferences.\n",
      "\n",
      "By addressing these areas, AI can make municipal services more efficient, responsive, and proactive, ultimately enhancing the quality of life for citizens while optimizing resource allocation.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "# Define test models and a consistent prompt\n",
    "models_to_test = [\n",
    "    \"llama3.1:latest\", # Follow the \"NAME\" shown in the output in the previous step\n",
    "    \"mistral:latest\",\n",
    "    \"EntropyYue/chatglm3:latest\",\n",
    "    \"deepseek-r1:7b\"\n",
    "]\n",
    "\n",
    "# Test prompt\n",
    "prompt = \"How can AI be used to enhance the municipal reporting service? Give me a clear and concise answer.\"\n",
    "\n",
    "# Dictionary to store results\n",
    "test_results = {}\n",
    "\n",
    "# Loop through models\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\nTesting model: {model_name}\")\n",
    "\n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\n",
    "            \"model\": model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Extract response\n",
    "    result = response.json()\n",
    "    output = result.get(\"response\", \"[No response returned]\")\n",
    "    elapsed = round(end_time - start_time, 2)\n",
    "\n",
    "    # Store in results dictionary\n",
    "    test_results[model_name] = {\n",
    "        \"response\": output,\n",
    "        \"inference_time_sec\": elapsed\n",
    "    }\n",
    "\n",
    "    print(f\"Inference Time: {elapsed} sec\")\n",
    "    print(f\"Response:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318800b5",
   "metadata": {},
   "source": [
    "Here are the initial observations from running the test prompt:\n",
    "\n",
    "| Model      | Inference Time (s) | Output Quality (Subjective) | Notes                                                                                                                                                                          |\n",
    "|------------|--------------------|-----------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| llama3.1 (8B)  | 🟨🟨🟨⬜⬜ 18.26              | 🟩🟩🟩🟩🟩 Excellent                   | Ideal for reasoning and chat replies, with decent speed.                                                              |\n",
    "| mistral    | 🟩🟩🟩🟩⬜ 13.96              | 🟩🟩🟩🟩⬜ Very Good                  | Very fast, could be a solid for classification. tasks                                                                                                                    |\n",
    "| chatglm3   | 🟩🟩🟩🟩🟩 8.51               | 🟨🟨🟨⬜⬜ Good                      | Extremely fast, good bilingual support; but content lacked depth.                            |\n",
    "| deepseek-r1 (7B)   | 🟥⬜⬜⬜⬜ 53.25              | 🟩🟩🟩🟩🟩 Excellent                 | Strong performance but concerning speed. Also offers thought process. |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
