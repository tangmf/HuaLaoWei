{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe6221b",
   "metadata": {},
   "source": [
    "# **6.0** ‎ Querying the LLM with RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be2178",
   "metadata": {},
   "source": [
    "### Purpose of This Notebook\n",
    "\n",
    "This notebook brings together all the previous components to form a complete **Retrieval-Augmented Generation (RAG) chatbot pipeline**. \\\n",
    "It demonstrates how to take a user query, retrieve relevant information from a vector database, optionally rerank, and generate a final grounded response using a LLM.\n",
    "\n",
    "This is the final stage of the RAG pipeline where we would want to see our municipal chatbot becomes fully functional for deployment use — \\\n",
    "capable of generating **context-aware**, **domain-specific** responses by combining LLM generation with trusted municipal documents.\n",
    "\n",
    "---\n",
    "\n",
    "### What This Notebook Covers\n",
    "\n",
    "- Accepting a user query as input\n",
    "- Retrieving relevant document chunks from the indexed vector database (Chroma)\n",
    "- Applying **Flashrank reranking** to improve relevance of results\n",
    "- Injecting the top-ranked context into a prompt template\n",
    "- Querying a **local or API-based LLM** (e.g., via Ollama or OpenAI)\n",
    "- Generating a final, human-readable response grounded in retrieved context\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Traditional LLMs often hallucinate or make up facts — especially in niche domains like municipal services. \\\n",
    "By grounding the LLM's response in documents retrieved from a trusted knowledge base, RAG improves both **accuracy** and **trustworthiness**. \\\n",
    "It ensures the chatbot answers queries with **real information**, not just guesses.\n",
    "\n",
    "---\n",
    "\n",
    "### Assumptions & Dependencies\n",
    "\n",
    "This notebook assumes you have already:\n",
    "- Indexed your documents in a vector store (Chroma)\n",
    "- Set up an embedding model and reranker (e.g., SentenceTransformer + Flashrank)\n",
    "- Integrated and tested your LLM (via LangChain, Ollama, or OpenAI)\n",
    "\n",
    "The output of this notebook is a working pipeline that can be wrapped into an API, chatbot interface, or web app.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5bc2fd",
   "metadata": {},
   "source": [
    "### **6.0.1** ‎ ‎ Load ChromaDB Collection and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "923ce638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Fleming Siow\\Documents\\GitHub\\HuaLaoWei\\ai_models\\chatbot\\venv\\Lib\\site-packages\\~hromadb_rust_bindings'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet chromadb sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcf7d91",
   "metadata": {},
   "source": [
    "# **6.1** ‎ Setting up the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89858286",
   "metadata": {},
   "source": [
    "### **6.1.1** ‎ ‎ Preparing the Voice Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b0e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from config import MODEL_PATHS\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "def transcribe(audio_path):\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\", task=\"transcribe\")\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "\n",
    "    model.generation_config.forced_decoder_ids = None\n",
    "\n",
    "    # Use GPU if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    # Load and preprocess audio\n",
    "    audio_array, sampling_rate = librosa.load(audio_path, sr=16000)  # Whisper expects 16kHz\n",
    "\n",
    "    # Prepare input features\n",
    "    inputs = processor(audio_array, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    input_features = inputs.input_features.to(device)\n",
    "\n",
    "    # Generate prediction\n",
    "    predicted_ids = model.generate(inputs[\"input_features\"])\n",
    "\n",
    "    # Decode transcription\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return transcription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f31da783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load from persisted ChromaDB directory\n",
    "chroma_client = chromadb.PersistentClient(path=\"../vector_stores/chroma_store_textonly\")\n",
    "collection = chroma_client.get_collection(name=\"municipal_issues\")\n",
    "\n",
    "# Load the embedding model (same one used during ingestion)\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5594c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "import chromadb\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a4c7bc",
   "metadata": {},
   "source": [
    "**Load ChromaDB Collection and Embedding Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa39edd",
   "metadata": {},
   "source": [
    "**Define Search Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c588921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_issues(query, k=5, metadata_filter=None):\n",
    "    embedding = embedding_model.encode(query).tolist()\n",
    "\n",
    "    # Add filter condition if needed\n",
    "    query_kwargs = {\n",
    "        \"query_embeddings\": [embedding],\n",
    "        \"n_results\": k,\n",
    "    }\n",
    "    if metadata_filter:\n",
    "        query_kwargs[\"where\"] = metadata_filter\n",
    "\n",
    "    results = collection.query(**query_kwargs)\n",
    "\n",
    "    documents = results[\"documents\"][0]\n",
    "    metadatas = results[\"metadatas\"][0]\n",
    "    return documents, metadatas                     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbef6de4",
   "metadata": {},
   "source": [
    "**Define a Context-Aware Prompt Template**\n",
    "\n",
    "An example prompt optimised for municipal service context, using in-context instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c930e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=textwrap.dedent(\"\"\"\\\n",
    "        You are a helpful municipal assistant in Singapore. Use the data below to answer the user's question accurately and concisely.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        User question: {question}\n",
    "        Answer:\"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33b108",
   "metadata": {},
   "source": [
    "**Query the System**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6df31ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_thought_and_answer(llm_output: str):\n",
    "    match = re.search(r\"<think>(.*?)</think>(.*)\", llm_output, re.DOTALL)\n",
    "    if match:\n",
    "        thought = match.group(1).strip()\n",
    "        answer = match.group(2).strip()\n",
    "        return thought, answer\n",
    "    else:\n",
    "        return None, llm_output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05a21781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM + RAG Response:\n",
      "Final Answer: Yes, there was a high-severity rodent issue reported at an establishment in Toa Payoh Central on April 13, 2025. The incident falls under the category of Pests > Rodents in Food Establishment and was resolved by the National Environment Agency (NEA).\n",
      "Reasoning (optional): Okay, so I need to figure out if there have been any recent high-severity rodent issues in Toa Payoh based on the data provided. Let me go through each entry step by step.\n",
      "\n",
      "First, I see that the user has shared several incidents reported between April 12 and April 15, all under different categories but all related to Toa Payoh Central area with coordinates (1.3321, 103.8478). \n",
      "\n",
      "Looking at each incident:\n",
      "\n",
      "1. The first entry from April 15 mentions rodent issues in the context of \"Foreign gun every field treatment.\" I'm not entirely sure what that means, but it might refer to something like rat traps or efforts to control rats.\n",
      "\n",
      "2. There's another incident on April 13 related to high severity pests, specifically rodents in a food establishment. This one is more directly about rats or mice in a place where food is handled, which could be a health concern.\n",
      "\n",
      "Then there are other entries from April 12 about different issues like magazine security, bicycles, drains, and abandoned trolleys, but only the second entry on April 13 specifically mentions rodents in a food establishment. The rest don't mention rodents at all.\n",
      "\n",
      "Since the user is asking about rodent issues, I should focus on that. Only one incident out of five is directly related to rodents, reported on April 13. It's categorized under Pests > Rodents in Food Establishment with high severity and status closed by NEA.\n",
      "\n",
      "I don't see any other incidents from Toa Payoh Central after April 15, so I can conclude that there was a recent high-severity rodent issue in late April.\n"
     ]
    }
   ],
   "source": [
    "llm = OllamaLLM(model=\"deepseek-r1:7b\")\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# Define user query\n",
    "user_query = \"Are there any recent high severity rodent issues in Toa Payoh?\"\n",
    "\n",
    "# Set filter (if needed)\n",
    "metadata_filter = {\n",
    "    \"$and\": [\n",
    "        {\"agency\": {\"$eq\": \"National Environment Agency (NEA)\"}},\n",
    "        {\"severity\": {\"$eq\": \"High\"}}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Get top matches from Chroma\n",
    "docs, metas = search_similar_issues(user_query, k=5, metadata_filter=metadata_filter)\n",
    "\n",
    "# Format retrieved context\n",
    "rag_context = \"\\n\\n---\\n\\n\".join(docs)\n",
    "\n",
    "# Generate final response\n",
    "response = chain.invoke({\"context\": rag_context, \"question\": user_query})\n",
    "thought, answer = split_thought_and_answer(response)\n",
    "print(\"LLM + RAG Response:\")\n",
    "print(\"Final Answer:\", answer)\n",
    "print(\"Reasoning (optional):\", thought)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
