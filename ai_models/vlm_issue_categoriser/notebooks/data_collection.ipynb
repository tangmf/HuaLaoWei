{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0baec7ed",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining three from git+https://github.com/codeforamerica/three.git@67b4a4b233a57aa7995d01f6b0f69c2e85aea6c0#egg=three (from -r requirements.txt (line 14))\n",
      "  Skipping because already up-to-date.\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting appdirs==1.4.3 (from -r requirements.txt (line 1))\n",
      "  Using cached appdirs-1.4.3-py2.py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting funcsigs==1.0.2 (from -r requirements.txt (line 2))\n",
      "  Using cached funcsigs-1.0.2-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting lxml>=4.9.3 (from -r requirements.txt (line 3))\n",
      "  Downloading lxml-5.3.2-cp39-cp39-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting mock==2.0.0 (from -r requirements.txt (line 4))\n",
      "  Using cached mock-2.0.0-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting nltk==3.2.2 (from -r requirements.txt (line 5))\n",
      "  Using cached nltk-3.2.2-py3-none-any.whl\n",
      "Collecting packaging==16.8 (from -r requirements.txt (line 6))\n",
      "  Using cached packaging-16.8-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting pbr==2.0.0 (from -r requirements.txt (line 7))\n",
      "  Using cached pbr-2.0.0-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pyparsing==2.2.0 (from -r requirements.txt (line 8))\n",
      "  Using cached pyparsing-2.2.0-py2.py3-none-any.whl.metadata (910 bytes)\n",
      "Collecting rake-nltk==1.0.0 (from -r requirements.txt (line 9))\n",
      "  Using cached rake_nltk-1.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting relaxml==0.1.3 (from -r requirements.txt (line 10))\n",
      "  Using cached relaxml-0.1.3-py3-none-any.whl\n",
      "Collecting requests==2.13.0 (from -r requirements.txt (line 11))\n",
      "  Using cached requests-2.13.0-py2.py3-none-any.whl.metadata (44 kB)\n",
      "Collecting simplejson==3.10.0 (from -r requirements.txt (line 12))\n",
      "  Using cached simplejson-3.10.0-cp39-cp39-win_amd64.whl\n",
      "Collecting six==1.10.0 (from -r requirements.txt (line 13))\n",
      "  Using cached six-1.10.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Using cached appdirs-1.4.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
      "Using cached mock-2.0.0-py2.py3-none-any.whl (56 kB)\n",
      "Using cached packaging-16.8-py2.py3-none-any.whl (23 kB)\n",
      "Using cached pbr-2.0.0-py2.py3-none-any.whl (98 kB)\n",
      "Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB)\n",
      "Using cached rake_nltk-1.0.0-py2.py3-none-any.whl (7.1 kB)\n",
      "Using cached requests-2.13.0-py2.py3-none-any.whl (584 kB)\n",
      "Using cached six-1.10.0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading lxml-5.3.2-cp39-cp39-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.8/3.8 MB 25.1 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: three\n",
      "  Building editable for three (pyproject.toml): started\n",
      "  Building editable for three (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for three: filename=three-0.8.0-0.editable-py3-none-any.whl size=3978 sha256=2176a82bbcc5c2c9687885cdcf4abd45b1fec027b5ee39c444d26d0a07800be0\n",
      "  Stored in directory: C:\\Users\\Jerick\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-j4fu0zqg\\wheels\\85\\bd\\04\\2246304c3d78dcdecc45f37da5b467165608deec899631fab1\n",
      "Successfully built three\n",
      "Installing collected packages: six, simplejson, requests, relaxml, pyparsing, pbr, funcsigs, appdirs, packaging, nltk, mock, lxml, three, rake-nltk\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.17.0\n",
      "    Uninstalling six-1.17.0:\n",
      "      Successfully uninstalled six-1.17.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.2\n",
      "    Uninstalling packaging-24.2:\n",
      "      Successfully uninstalled packaging-24.2\n",
      "Successfully installed appdirs-1.4.3 funcsigs-1.0.2 lxml-5.3.2 mock-2.0.0 nltk-3.2.2 packaging-16.8 pbr-2.0.0 pyparsing-2.2.0 rake-nltk-1.0.0 relaxml-0.1.3 requests-2.13.0 simplejson-3.10.0 six-1.10.0 three-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a47d2c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import urllib\n",
    "from tqdm import tqdm\n",
    "from three import Three\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176fbbe6",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b1734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FMS_BASE_URL = \"https://www.fixmystreet.com/open311/v2/\"\n",
    "FMS_BASE_PARAMETERS = {\n",
    "    \"jurisdiction_id\": \"fixmystreet\",\n",
    "}\n",
    "DATA_FOLDER = \"../data/1. Original\"\n",
    "\n",
    "CATEGORIES = [\n",
    "    \"Abandoned vehicles\",\n",
    "    \"Blocked drainage gullies\",\n",
    "    \"Bus stops\",\n",
    "    \"Car parking\",\n",
    "    \"Dog fouling\",\n",
    "    \"Flyposting\",\n",
    "    \"Flytipping\",\n",
    "    \"Graffiti\",\n",
    "    \"Parks/landscapes\",\n",
    "    \"Pavements/footpaths\",\n",
    "    \"Potholes\",\n",
    "    \"Public toilets\",\n",
    "    \"Roads/highways\",\n",
    "    \"Road traffic signs\",\n",
    "    \"Rubbish (refuse and recycling)\",\n",
    "    \"Street cleaning\",\n",
    "    \"Street lighting\",\n",
    "    \"Street nameplates\",\n",
    "    \"Traffic lights\",\n",
    "    \"Trees\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b955d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMS(Three):\n",
    "    def __init__(self):\n",
    "        super(FMS, self).__init__()\n",
    "        self.endpoint = FMS_BASE_URL\n",
    "        self.format = \"xml\"\n",
    "        self.jurisdiction = FMS_BASE_PARAMETERS['jurisdiction_id']\n",
    "\n",
    "\n",
    "def make_a_category_folder(category):\n",
    "    folder_path = os.path.join(DATA_FOLDER, category)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "\n",
    "def setup_folders():\n",
    "    \"\"\" Initialise the folder structure for training if it doesn't exist\"\"\"\n",
    "    if not os.path.exists(DATA_FOLDER):\n",
    "        os.makedirs(DATA_FOLDER)\n",
    "    for cat in CATEGORIES:\n",
    "        cat = re.sub(r'[<>:\"/\\\\|?*]', ' ', cat)\n",
    "        make_a_category_folder(cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c1dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANT_KEYS = {\n",
    "    'name', 'place', 'amenity', 'landuse', 'leisure', 'building',\n",
    "    'highway', 'natural', 'shop', 'tourism', 'man_made', 'railway'\n",
    "}\n",
    "\n",
    "def filter_tags(tags: list[dict], useful_keys: set[str]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Filter a list of tag dictionaries to keep only useful keys.\n",
    "\n",
    "    Parameters:\n",
    "        tags (list[dict]): A list of dictionaries containing tag key-value pairs.\n",
    "        useful_keys (set[str]): A set of tag keys to keep.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A new list of dictionaries with only the useful keys retained.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        filtered_tag\n",
    "        for tag in tags\n",
    "        if (filtered_tag := {k: v for k, v in tag.items() if k in useful_keys})\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_osm_tags_from_openstreetmap(lat, lon, radius=15, filter=True):\n",
    "    overpass_url = \"https://overpass-api.de/api/interpreter\"\n",
    "    tags = {\"nearby\": [], \"enclosing\": []}\n",
    "\n",
    "    if not isinstance(lat, (int, float)) or not isinstance(lon, (int, float)):\n",
    "        print(\"Invalid latitude or longitude values.\")\n",
    "        return tags\n",
    "    \n",
    "    # Define bounding box for out+geom (slightly expanded around the point)\n",
    "    lat_min = lat - 0.0015\n",
    "    lat_max = lat + 0.0015\n",
    "    lon_min = lon - 0.0015\n",
    "    lon_max = lon + 0.0015\n",
    "    bbox = f\"{lat_min},{lon_min},{lat_max},{lon_max}\"\n",
    "\n",
    "    # Nearby query\n",
    "    nearby_query = f\"\"\"\n",
    "    [timeout:10][out:json];\n",
    "    (\n",
    "      node(around:{radius},{lat},{lon});\n",
    "      way(around:{radius},{lat},{lon});\n",
    "    );\n",
    "    out tags geom({bbox});\n",
    "    relation(around:{radius},{lat},{lon});\n",
    "    out geom({bbox});\n",
    "    \"\"\"\n",
    "\n",
    "    # Enclosing query\n",
    "    enclosing_query = f\"\"\"\n",
    "    [timeout:10][out:json];\n",
    "    is_in({lat},{lon})->.a;\n",
    "    way(pivot.a);\n",
    "    out tags bb;\n",
    "    out ids geom({bbox});\n",
    "    relation(pivot.a);\n",
    "    out tags bb;\n",
    "    \"\"\"\n",
    "\n",
    "    def run_query(query):\n",
    "        response = requests.post(overpass_url, data={\"data\": query})\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "    try:\n",
    "        nearby_data = run_query(nearby_query)\n",
    "        enclosing_data = run_query(enclosing_query)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error querying Overpass API:\", e)\n",
    "        return tags\n",
    "\n",
    "\n",
    "    # Collect tags from nearby results\n",
    "    for element in nearby_data.get(\"elements\", []):\n",
    "        if \"tags\" in element:\n",
    "            tags[\"nearby\"].append(element[\"tags\"])\n",
    "\n",
    "    # Collect tags from enclosing results\n",
    "    for element in enclosing_data.get(\"elements\", []):\n",
    "        if \"tags\" in element:\n",
    "            tags[\"enclosing\"].append(element[\"tags\"])\n",
    "\n",
    "    if filter:\n",
    "        tags[\"nearby\"] = filter_tags(tags[\"nearby\"], RELEVANT_KEYS)\n",
    "        tags[\"enclosing\"] = filter_tags(tags[\"enclosing\"], RELEVANT_KEYS)\n",
    "\n",
    "    return tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a272522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_report_data(report):\n",
    "    \"\"\" Given an single XML entry representing a single FMS report,\n",
    "    grab the metadata and attached photo(s) placing in the correct folder\n",
    "    to build the dataset for training.\"\"\"\n",
    "    if 'media_url' in report.keys():\n",
    "        web_path = urlparse(report['media_url'])\n",
    "        file_name = os.path.split(web_path.path)[1]\n",
    "\n",
    "        report['service_code'] = re.sub(r'[<>:\"/\\\\|?*]', ' ', report['service_code'])\n",
    "\n",
    "        img_file_path = os.path.join(DATA_FOLDER,\n",
    "                                     report['service_code'],\n",
    "                                     file_name)\n",
    "        print(report['service_code'])\n",
    "\n",
    "        try:\n",
    "            urlretrieve(report['media_url'], img_file_path)\n",
    "        except FileNotFoundError as e:\n",
    "            print(\"New service code found, because we're special\")\n",
    "            make_a_category_folder(report['service_code'])\n",
    "            urlretrieve(report['media_url'], img_file_path)\n",
    "\n",
    "        label_file_name = file_name.rsplit('.', maxsplit=1)[0] + \".json\"\n",
    "        labels_file_path = os.path.join(DATA_FOLDER,\n",
    "                                        report['service_code'],\n",
    "                                        label_file_name)\n",
    "        \n",
    "        print(labels_file_path)\n",
    "\n",
    "        tags = get_osm_tags_from_openstreetmap(float(report[\"lat\"]), float(report[\"long\"]))\n",
    "\n",
    "        with open(labels_file_path, 'w', encoding=\"utf-8\") as f:\n",
    "            json.dump({\"description\": report['description'], \"tags\": tags, \"lat\": report['lat'], \"long\": report['long']}, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce905372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create intial folders (skip if already done)\n",
    "setup_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6f17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fms_client = FMS()\n",
    "\n",
    "# Start/End date and how many days to grab at once\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime.today()\n",
    "delta = timedelta(days=7)\n",
    "\n",
    "# Query API to collect data\n",
    "while start_date < end_date:\n",
    "    next_date = start_date + delta\n",
    "    date_range = [start_date.strftime(\"%m-%d-%Y\"), next_date.strftime(\"%m-%d-%Y\")]\n",
    "    print(f\"Fetching between: {date_range[0]} to {date_range[1]}\")\n",
    "\n",
    "    try:\n",
    "        response = fms_client.requests(between=date_range, count=1000)\n",
    "        reports = response.get('request', [])\n",
    "        if not reports:\n",
    "            print(\"No reports found in this range.\")\n",
    "        for report in reports:\n",
    "            grab_report_data(report)\n",
    "    except Exception as e:\n",
    "        print(f\"Error for date range {date_range}: {e}\")\n",
    "\n",
    "    start_date = next_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8215334",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac4ce68",
   "metadata": {},
   "source": [
    "Data was manually sorted to ensure accuracy. Raw data was extremely messy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "624d66f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_label_vectors_with_images(data_root):\n",
    "    \"\"\"\n",
    "    Build a dictionary mapping report_id to its labels (categories),\n",
    "    and include whether the JSON file has an associated image and its path.\n",
    "    The categories are ordered by their hierarchy, with higher-level categories appearing first.\n",
    "    \"\"\"\n",
    "    label_map = defaultdict(lambda: {\"labels\": [], \"severity\": \"\", \"image_path\": None})\n",
    "\n",
    "    for root, _, files in os.walk(data_root):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                report_id = os.path.splitext(file)[0]\n",
    "\n",
    "                # Relative path from data_root\n",
    "                rel_path = os.path.relpath(root, data_root)\n",
    "                # Each part of the relative path is a label\n",
    "                categories = rel_path.split(os.sep)\n",
    "\n",
    "                # Ensure categories are ordered by hierarchy (top-level first)\n",
    "                categories = sorted(categories, key=lambda x: rel_path.index(x))\n",
    "\n",
    "                # Check for associated image\n",
    "                image_extensions = ['.jpeg', '.jpg', '.png']\n",
    "                image_path = None\n",
    "                for ext in image_extensions:\n",
    "                    potential_image_path = os.path.normpath(os.path.join(root, report_id + ext))\n",
    "                    if os.path.exists(potential_image_path):\n",
    "                        image_path = potential_image_path\n",
    "                        break\n",
    "\n",
    "                # Update label map\n",
    "                label_map[report_id][\"labels\"].extend(categories)\n",
    "                label_map[report_id][\"labels\"] = list(dict.fromkeys(label_map[report_id][\"labels\"]))  # Remove duplicates while preserving order\n",
    "                label_map[report_id][\"severity\"] = \"Low\" # Default severity\n",
    "                label_map[report_id][\"image_path\"] = image_path or label_map[report_id][\"image_path\"]  # None if no image found\n",
    "            \n",
    "    return label_map\n",
    "\n",
    "# Example usage\n",
    "data_root = \"./data/2. Sorted\"\n",
    "label_vectors = build_label_vectors_with_images(data_root)\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"label_vectors.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(label_vectors, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18248eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../data/2. Sorted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8c4feaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 7284551.0.full\n"
     ]
    }
   ],
   "source": [
    "# finding the file with the most classes (for checking)\n",
    "\n",
    "with open(\"label_vectors.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    max_count = 0\n",
    "    path_name = \"\"\n",
    "\n",
    "    data = json.load(f)\n",
    "    for k, v in data.items():\n",
    "        if len(v[\"labels\"])>max_count:\n",
    "            max_count = len(v[\"labels\"])\n",
    "            path_name = k\n",
    "\n",
    "print(max_count, path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cab9656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(folder_path, \"./Smoking/Other Public Areas/singapore-singapore-people-smoking-in-front-of-a-shopping-center-P24BYK.json\")\n",
    "lat = 1.2881984975229688\n",
    "long = 103.84640096944811\n",
    "description = \"I hate smoke. Please enforce the no smoking law.\"\n",
    "\n",
    "if lat or long:\n",
    "    lat = round(float(lat), 6)\n",
    "    long = round(float(long), 6)\n",
    "\n",
    "tags = get_osm_tags_from_openstreetmap(lat, long)\n",
    "\n",
    "with open(file_path, 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump({\"description\": description, \"tags\": tags, \"lat\": lat, \"long\": long}, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
